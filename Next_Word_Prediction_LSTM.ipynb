{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Load txt file\n",
        "The txt file is collected from https://www.gutenberg.org/files/84/84-h/84-h.htm"
      ],
      "metadata": {
        "id": "dmRsr1v_R8vh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ck95wL8qp-gb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0b94d219-63a4-4708-c99f-710f248c8c0d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6b33acab-48e1-4c8b-9699-39d5c5e65322\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6b33acab-48e1-4c8b-9699-39d5c5e65322\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Frankenstein.txt to Frankenstein.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "file = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "xEgRX-ObJa92"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Open and pre-process the data"
      ],
      "metadata": {
        "id": "1C2YWLoQSNx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('Frankenstein.txt', 'r')\n",
        "\n",
        "#Store data of the txt file in a list\n",
        "doc = []\n",
        "for i in file:\n",
        "  doc.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in doc:\n",
        "  data = ' '. join(doc) \n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\\\n",
        "            .replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space"
      ],
      "metadata": {
        "id": "mFg3ipPurKNJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove unnecessary spaces \n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "print(data[:100])\n",
        "print(f\"Total number of words is {len(data)}\")"
      ],
      "metadata": {
        "id": "3m6HlqtbJ7-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2087870c-0785-471b-f2a5-b540557e4e31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelley This eBook is f\n",
            "Total number of words is 436916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply tokenization and some other changes"
      ],
      "metadata": {
        "id": "2_eswFthS-oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "data_sequence = tokenizer.texts_to_sequences([data])[0]\n",
        "print(data_sequence[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmoOh0LwS9nq",
        "outputId": "f5b625fc-dffc-48db-ce6a-d864a3af4317"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 99, 86, 683, 4, 301, 23, 2353, 2354, 2355, 2356, 25, 683, 31, 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of data_sequence is {len(data_sequence)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfOUiMZwS9qg",
        "outputId": "6675f4a1-37da-42ea-ac3f-41746868661f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data_sequence is 78466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tri-gram based model"
      ],
      "metadata": {
        "id": "-2EszSgXTru1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y  = [], []\n",
        "k = 3\n",
        "for i in range(k, len(data_sequence)):\n",
        "    words = data_sequence[i-k:i+1]\n",
        "    X.append(words[: k])\n",
        "    y.append(words[k])\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(X))\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(f\"Data : \\n{X[:10]}\")\n",
        "print(f\"Response : \\n{y[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvOan7sDS9sy",
        "outputId": "d1ef4749-1fe6-46ef-ce03-314d84ff430a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  78463\n",
            "Data : \n",
            "[[   1   99   86]\n",
            " [  99   86  683]\n",
            " [  86  683    4]\n",
            " [ 683    4  301]\n",
            " [   4  301   23]\n",
            " [ 301   23 2353]\n",
            " [  23 2353 2354]\n",
            " [2353 2354 2355]\n",
            " [2354 2355 2356]\n",
            " [2355 2356   25]]\n",
            "Response : \n",
            "[ 683    4  301   23 2353 2354 2355 2356   25  683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 # 0 is reserved for padding and that's why we add 1\n",
        "print(f\"No of different words is {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq6oXjHxS9vO",
        "outputId": "d3f1b3b7-f128-4501-a447-9d35ec77ac67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of different words is 7495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes = vocab_size)\n",
        "print(y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkpcnlhkS9yb",
        "outputId": "b48b255d-b19a-48ec-c87a-5cb4fa72c9b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fit LSTM Model"
      ],
      "metadata": {
        "id": "whJmgP25TvUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(name='LSTM')\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOwtfz5_TKFS",
        "outputId": "b948b07e-22a7-4bfa-bb27-81c6c0f858a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             74950     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 512)            1071104   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 512)               2099200   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7495)              3844935   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,352,845\n",
            "Trainable params: 7,352,845\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the model"
      ],
      "metadata": {
        "id": "RgduyF1QTefo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "YuP52VNVTKKx",
        "outputId": "42d48977-23c9-4f51-832b-0aca50372112"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint('next_words.h5', monitor = 'loss', verbose = 1, save_best_only = True)"
      ],
      "metadata": {
        "id": "K5ASm5BfTKNL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit(X, y, epochs = 85, batch_size = 64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLEDzSnjTKP9",
        "outputId": "d2b02ab7-96b5-4e8d-c1e9-9acc68e6f542"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 6.6283 - accuracy: 0.0614\n",
            "Epoch 1: loss improved from inf to 6.62825, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 19s 9ms/step - loss: 6.6283 - accuracy: 0.0614\n",
            "Epoch 2/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 6.1224 - accuracy: 0.0911\n",
            "Epoch 2: loss improved from 6.62825 to 6.12241, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 6.1224 - accuracy: 0.0911\n",
            "Epoch 3/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 5.7620 - accuracy: 0.1167\n",
            "Epoch 3: loss improved from 6.12241 to 5.76160, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 5.7616 - accuracy: 0.1168\n",
            "Epoch 4/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 5.4719 - accuracy: 0.1322\n",
            "Epoch 4: loss improved from 5.76160 to 5.47128, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 5.4713 - accuracy: 0.1323\n",
            "Epoch 5/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 5.2404 - accuracy: 0.1450\n",
            "Epoch 5: loss improved from 5.47128 to 5.24037, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 5.2404 - accuracy: 0.1450\n",
            "Epoch 6/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 5.0347 - accuracy: 0.1549\n",
            "Epoch 6: loss improved from 5.24037 to 5.03516, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 5.0352 - accuracy: 0.1548\n",
            "Epoch 7/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 4.8300 - accuracy: 0.1624\n",
            "Epoch 7: loss improved from 5.03516 to 4.83003, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 4.8300 - accuracy: 0.1624\n",
            "Epoch 8/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 4.6105 - accuracy: 0.1711\n",
            "Epoch 8: loss improved from 4.83003 to 4.61115, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 4.6111 - accuracy: 0.1710\n",
            "Epoch 9/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 4.3761 - accuracy: 0.1808\n",
            "Epoch 9: loss improved from 4.61115 to 4.37608, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 4.3761 - accuracy: 0.1807\n",
            "Epoch 10/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 4.1286 - accuracy: 0.1924\n",
            "Epoch 10: loss improved from 4.37608 to 4.12857, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 4.1286 - accuracy: 0.1924\n",
            "Epoch 11/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 3.8678 - accuracy: 0.2114\n",
            "Epoch 11: loss improved from 4.12857 to 3.86769, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 3.8677 - accuracy: 0.2115\n",
            "Epoch 12/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 3.6058 - accuracy: 0.2396\n",
            "Epoch 12: loss improved from 3.86769 to 3.60584, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 3.6058 - accuracy: 0.2396\n",
            "Epoch 13/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 3.3544 - accuracy: 0.2713\n",
            "Epoch 13: loss improved from 3.60584 to 3.35490, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 3.3549 - accuracy: 0.2711\n",
            "Epoch 14/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 3.1182 - accuracy: 0.3048\n",
            "Epoch 14: loss improved from 3.35490 to 3.11797, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 3.1180 - accuracy: 0.3049\n",
            "Epoch 15/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 2.8966 - accuracy: 0.3395\n",
            "Epoch 15: loss improved from 3.11797 to 2.89698, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 2.8970 - accuracy: 0.3393\n",
            "Epoch 16/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 2.6944 - accuracy: 0.3715\n",
            "Epoch 16: loss improved from 2.89698 to 2.69453, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 2.6945 - accuracy: 0.3715\n",
            "Epoch 17/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 2.4980 - accuracy: 0.4041\n",
            "Epoch 17: loss improved from 2.69453 to 2.49837, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 2.4984 - accuracy: 0.4040\n",
            "Epoch 18/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 2.3124 - accuracy: 0.4379\n",
            "Epoch 18: loss improved from 2.49837 to 2.31329, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 2.3133 - accuracy: 0.4377\n",
            "Epoch 19/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 2.1324 - accuracy: 0.4732\n",
            "Epoch 19: loss improved from 2.31329 to 2.13253, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 2.1325 - accuracy: 0.4731\n",
            "Epoch 20/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 1.9674 - accuracy: 0.5052\n",
            "Epoch 20: loss improved from 2.13253 to 1.96772, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 1.9677 - accuracy: 0.5051\n",
            "Epoch 21/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 1.8064 - accuracy: 0.5369\n",
            "Epoch 21: loss improved from 1.96772 to 1.80631, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 1.8063 - accuracy: 0.5369\n",
            "Epoch 22/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 1.6573 - accuracy: 0.5727\n",
            "Epoch 22: loss improved from 1.80631 to 1.65735, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 1.6573 - accuracy: 0.5727\n",
            "Epoch 23/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.6031\n",
            "Epoch 23: loss improved from 1.65735 to 1.51961, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 1.5196 - accuracy: 0.6029\n",
            "Epoch 24/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 1.3893 - accuracy: 0.6354\n",
            "Epoch 24: loss improved from 1.51961 to 1.38920, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.3892 - accuracy: 0.6355\n",
            "Epoch 25/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 1.2731 - accuracy: 0.6610\n",
            "Epoch 25: loss improved from 1.38920 to 1.27312, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.2731 - accuracy: 0.6610\n",
            "Epoch 26/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 1.1658 - accuracy: 0.6886\n",
            "Epoch 26: loss improved from 1.27312 to 1.16668, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 1.1667 - accuracy: 0.6883\n",
            "Epoch 27/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 1.0642 - accuracy: 0.7140\n",
            "Epoch 27: loss improved from 1.16668 to 1.06495, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 1.0650 - accuracy: 0.7138\n",
            "Epoch 28/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.9800 - accuracy: 0.7363\n",
            "Epoch 28: loss improved from 1.06495 to 0.98042, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9804 - accuracy: 0.7361\n",
            "Epoch 29/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.7555\n",
            "Epoch 29: loss improved from 0.98042 to 0.90599, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.9060 - accuracy: 0.7554\n",
            "Epoch 30/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.8316 - accuracy: 0.7738\n",
            "Epoch 30: loss improved from 0.90599 to 0.83157, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8316 - accuracy: 0.7738\n",
            "Epoch 31/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.7764 - accuracy: 0.7886\n",
            "Epoch 31: loss improved from 0.83157 to 0.77685, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7768 - accuracy: 0.7886\n",
            "Epoch 32/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.7288 - accuracy: 0.8011\n",
            "Epoch 32: loss improved from 0.77685 to 0.72881, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7288 - accuracy: 0.8010\n",
            "Epoch 33/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.6834 - accuracy: 0.8131\n",
            "Epoch 33: loss improved from 0.72881 to 0.68447, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6845 - accuracy: 0.8127\n",
            "Epoch 34/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.6455 - accuracy: 0.8219\n",
            "Epoch 34: loss improved from 0.68447 to 0.64583, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6458 - accuracy: 0.8218\n",
            "Epoch 35/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 0.6183 - accuracy: 0.8292\n",
            "Epoch 35: loss improved from 0.64583 to 0.61871, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6187 - accuracy: 0.8291\n",
            "Epoch 36/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.8367\n",
            "Epoch 36: loss improved from 0.61871 to 0.58870, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5887 - accuracy: 0.8365\n",
            "Epoch 37/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.8418\n",
            "Epoch 37: loss improved from 0.58870 to 0.56850, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5685 - accuracy: 0.8417\n",
            "Epoch 38/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.5486 - accuracy: 0.8462\n",
            "Epoch 38: loss improved from 0.56850 to 0.54882, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5488 - accuracy: 0.8462\n",
            "Epoch 39/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.5330 - accuracy: 0.8491\n",
            "Epoch 39: loss improved from 0.54882 to 0.53328, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5333 - accuracy: 0.8490\n",
            "Epoch 40/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.8539\n",
            "Epoch 40: loss improved from 0.53328 to 0.51061, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5106 - accuracy: 0.8539\n",
            "Epoch 41/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.8578\n",
            "Epoch 41: loss improved from 0.51061 to 0.49762, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4976 - accuracy: 0.8578\n",
            "Epoch 42/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.4849 - accuracy: 0.8595\n",
            "Epoch 42: loss improved from 0.49762 to 0.48485, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4849 - accuracy: 0.8595\n",
            "Epoch 43/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.8618\n",
            "Epoch 43: loss improved from 0.48485 to 0.47421, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4742 - accuracy: 0.8618\n",
            "Epoch 44/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.4685 - accuracy: 0.8631\n",
            "Epoch 44: loss improved from 0.47421 to 0.46900, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4690 - accuracy: 0.8631\n",
            "Epoch 45/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.8639\n",
            "Epoch 45: loss improved from 0.46900 to 0.45963, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4596 - accuracy: 0.8639\n",
            "Epoch 46/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.8666\n",
            "Epoch 46: loss improved from 0.45963 to 0.44468, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4447 - accuracy: 0.8666\n",
            "Epoch 47/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.8687\n",
            "Epoch 47: loss improved from 0.44468 to 0.43823, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4382 - accuracy: 0.8686\n",
            "Epoch 48/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 0.4353 - accuracy: 0.8674\n",
            "Epoch 48: loss improved from 0.43823 to 0.43551, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4355 - accuracy: 0.8673\n",
            "Epoch 49/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8699\n",
            "Epoch 49: loss improved from 0.43551 to 0.42038, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4204 - accuracy: 0.8699\n",
            "Epoch 50/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.4222 - accuracy: 0.8694\n",
            "Epoch 50: loss did not improve from 0.42038\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4222 - accuracy: 0.8694\n",
            "Epoch 51/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.4166 - accuracy: 0.8695\n",
            "Epoch 51: loss improved from 0.42038 to 0.41661, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4166 - accuracy: 0.8695\n",
            "Epoch 52/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8714\n",
            "Epoch 52: loss improved from 0.41661 to 0.40461, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4046 - accuracy: 0.8714\n",
            "Epoch 53/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 0.3991 - accuracy: 0.8737\n",
            "Epoch 53: loss improved from 0.40461 to 0.39930, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3993 - accuracy: 0.8736\n",
            "Epoch 54/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8740\n",
            "Epoch 54: loss improved from 0.39930 to 0.39376, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3938 - accuracy: 0.8740\n",
            "Epoch 55/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3930 - accuracy: 0.8734\n",
            "Epoch 55: loss improved from 0.39376 to 0.39304, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3930 - accuracy: 0.8733\n",
            "Epoch 56/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.3865 - accuracy: 0.8734\n",
            "Epoch 56: loss improved from 0.39304 to 0.38674, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3867 - accuracy: 0.8733\n",
            "Epoch 57/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8750\n",
            "Epoch 57: loss improved from 0.38674 to 0.37971, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3797 - accuracy: 0.8749\n",
            "Epoch 58/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8752\n",
            "Epoch 58: loss improved from 0.37971 to 0.37661, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3766 - accuracy: 0.8751\n",
            "Epoch 59/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8751\n",
            "Epoch 59: loss improved from 0.37661 to 0.37394, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3739 - accuracy: 0.8751\n",
            "Epoch 60/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8763\n",
            "Epoch 60: loss improved from 0.37394 to 0.36960, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3696 - accuracy: 0.8763\n",
            "Epoch 61/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.8753\n",
            "Epoch 61: loss improved from 0.36960 to 0.36769, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3677 - accuracy: 0.8753\n",
            "Epoch 62/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.8762\n",
            "Epoch 62: loss improved from 0.36769 to 0.36248, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3625 - accuracy: 0.8762\n",
            "Epoch 63/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8765\n",
            "Epoch 63: loss improved from 0.36248 to 0.35835, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3584 - accuracy: 0.8765\n",
            "Epoch 64/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8766\n",
            "Epoch 64: loss improved from 0.35835 to 0.35728, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3573 - accuracy: 0.8766\n",
            "Epoch 65/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8769\n",
            "Epoch 65: loss improved from 0.35728 to 0.35435, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3543 - accuracy: 0.8769\n",
            "Epoch 66/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3459 - accuracy: 0.8784\n",
            "Epoch 66: loss improved from 0.35435 to 0.34595, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3459 - accuracy: 0.8784\n",
            "Epoch 67/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8769\n",
            "Epoch 67: loss did not improve from 0.34595\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3502 - accuracy: 0.8769\n",
            "Epoch 68/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8780\n",
            "Epoch 68: loss improved from 0.34595 to 0.34270, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3427 - accuracy: 0.8779\n",
            "Epoch 69/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.8760\n",
            "Epoch 69: loss did not improve from 0.34270\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3466 - accuracy: 0.8760\n",
            "Epoch 70/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8780\n",
            "Epoch 70: loss improved from 0.34270 to 0.33759, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3376 - accuracy: 0.8780\n",
            "Epoch 71/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8784\n",
            "Epoch 71: loss improved from 0.33759 to 0.33178, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3318 - accuracy: 0.8784\n",
            "Epoch 72/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8798\n",
            "Epoch 72: loss did not improve from 0.33178\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3322 - accuracy: 0.8798\n",
            "Epoch 73/85\n",
            "1223/1226 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8787\n",
            "Epoch 73: loss did not improve from 0.33178\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3326 - accuracy: 0.8787\n",
            "Epoch 74/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8787\n",
            "Epoch 74: loss improved from 0.33178 to 0.33018, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3302 - accuracy: 0.8786\n",
            "Epoch 75/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8795\n",
            "Epoch 75: loss improved from 0.33018 to 0.32545, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3255 - accuracy: 0.8794\n",
            "Epoch 76/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8779\n",
            "Epoch 76: loss did not improve from 0.32545\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3275 - accuracy: 0.8778\n",
            "Epoch 77/85\n",
            "1225/1226 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8791\n",
            "Epoch 77: loss improved from 0.32545 to 0.32025, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3203 - accuracy: 0.8791\n",
            "Epoch 78/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8791\n",
            "Epoch 78: loss did not improve from 0.32025\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3206 - accuracy: 0.8791\n",
            "Epoch 79/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8798\n",
            "Epoch 79: loss improved from 0.32025 to 0.31846, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3185 - accuracy: 0.8797\n",
            "Epoch 80/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.8801\n",
            "Epoch 80: loss improved from 0.31846 to 0.31516, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3152 - accuracy: 0.8801\n",
            "Epoch 81/85\n",
            "1226/1226 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.8788\n",
            "Epoch 81: loss did not improve from 0.31516\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3167 - accuracy: 0.8788\n",
            "Epoch 82/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8786\n",
            "Epoch 82: loss did not improve from 0.31516\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3175 - accuracy: 0.8785\n",
            "Epoch 83/85\n",
            "1224/1226 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8807\n",
            "Epoch 83: loss improved from 0.31516 to 0.30659, saving model to next_words.h5\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3066 - accuracy: 0.8806\n",
            "Epoch 84/85\n",
            "1221/1226 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8811\n",
            "Epoch 84: loss did not improve from 0.30659\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3073 - accuracy: 0.8811\n",
            "Epoch 85/85\n",
            "1222/1226 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8814\n",
            "Epoch 85: loss did not improve from 0.30659\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3080 - accuracy: 0.8812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e34df6410>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the model"
      ],
      "metadata": {
        "id": "mpRr9OQHUvwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  text = input('Please enter sentence(Please write more than 2 words), 0 to exit: ')\n",
        "  if text != '0':\n",
        "    try:\n",
        "      text = text.split(' ')\n",
        "      text = text[-3: ]\n",
        "      sequence = np.array(tokenizer.texts_to_sequences([text]))\n",
        "\n",
        "      output = np.argpartition(model.predict(sequence)[0], -5)[-5:]\n",
        "      preds = np.argmax(model.predict(sequence))\n",
        "      predicted_word = \"\"    \n",
        "\n",
        "      next_words = []\n",
        "      for key, value in tokenizer.word_index.items():\n",
        "        if value in output:\n",
        "          next_words.append(key)\n",
        "\n",
        "        if value == preds:\n",
        "          predicted_word = key\n",
        "\n",
        "        if len(next_words) == 5:\n",
        "          break\n",
        "      print(f\"Possible predicted words:\\n{next_words}\")\n",
        "      print(f\"Most predicted word: {predicted_word}\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEg1-MmoTKSX",
        "outputId": "98650923-4ad4-46ad-f698-0deb35fc8b49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter sentence(Please write more than 2 words), 0 to exit: Yet do not suppose\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Possible predicted words:\n",
            "['who', 'possessed', 'however', 'necessary', 'because']\n",
            "Most predicted word: possessed\n",
            "Please enter sentence(Please write more than 2 words), 0 to exit: My swelling heart\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Possible predicted words:\n",
            "['and', 'of', 'to', 'in', 'involuntarily']\n",
            "Most predicted word: involuntarily\n",
            "Please enter sentence(Please write more than 2 words), 0 to exit: Why not still proceed over the untamed yet obedient element? What can\n",
            "1/1 [==============================] - 1s 866ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "Possible predicted words:\n",
            "['i', 'my', 'you', 'be', 'have']\n",
            "Most predicted word: be\n",
            "Please enter sentence(Please write more than 2 words), 0 to exit: 0\n"
          ]
        }
      ]
    }
  ]
}